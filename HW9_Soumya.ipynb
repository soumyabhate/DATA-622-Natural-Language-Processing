{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## DATA 622 Natural Language Processing\n",
        "### Homework 9"
      ],
      "metadata": {
        "id": "rZbn0WqnAuh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions\n",
        "\n",
        "Use the article https://apnews.com/article/boeing-aviation-aircraft-air-india-crashf12b20e65dc57ae655a1e0759b58938f.\n",
        "1. Classify the sentiment, the intent, and the emotions.\n",
        "2. Determine how much the article is about technology, aviation, and policies.\n",
        "3. Use LLMs and one Deep Learning method of your choice to answer the questions.\n",
        "Compare the results."
      ],
      "metadata": {
        "id": "cJFb3FCgBYxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \"requests==2.32.4\" beautifulsoup4 scikit-learn\n",
        "%pip install -q --index-url https://download.pytorch.org/whl/cpu torch==2.8.0+cpu\n",
        "%pip install -q \"transformers==4.45.2\" \"sentence-transformers==3.0.1\"\n",
        "\n",
        "# quick sanity print\n",
        "import torch, transformers, sentence_transformers\n",
        "print(\"torch:\", torch.__version__, \"| transformers:\", transformers.__version__, \"| sbert:\", sentence_transformers.__version__)"
      ],
      "metadata": {
        "id": "06MiayeId0fr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44999723-bd1f-4f5e-c39b-5bbe3606a354"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.8.0+cpu | transformers: 4.57.1 | sbert: 5.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Setup & fetch ============================================================\n",
        "import re, textwrap, numpy as np, requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "URL = \"https://apnews.com/article/boeing-aviation-aircraft-air-india-crashf12b20e65dc57ae655a1e0759b58938f\"\n",
        "\n",
        "def fetch_clean(url: str) -> str:\n",
        "    r = requests.get(url, timeout=30); r.raise_for_status()\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    for bad in soup([\"script\",\"style\",\"noscript\"]): bad.decompose()\n",
        "    text = \" \".join(soup.get_text(\" \").split())\n",
        "    return re.sub(r\"\\s+\",\" \", text).strip()\n",
        "\n",
        "doc = fetch_clean(URL)\n",
        "\n",
        "print(\"=\"*100)\n",
        "print(\"Source\")\n",
        "print(\"-\"*100)\n",
        "print(f\"URL: {URL}\\nCharacters: {len(doc):,}\\n\")\n",
        "\n",
        "# === Helpers =================================================================\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "def summarize(text, max_sent=3):\n",
        "    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text) if 40<=len(s)<=500]\n",
        "    if not sents: return \"\"\n",
        "    if len(sents) <= max_sent: return \" \".join(sents)\n",
        "    V = TfidfVectorizer(stop_words=\"english\", max_features=8000); S = V.fit_transform(sents)\n",
        "    score = (S.power(2).sum(axis=1)).A.ravel(); ix=np.argsort(score)[::-1][:max_sent]; ix.sort()\n",
        "    return \" \".join([sents[i] for i in ix])\n",
        "\n",
        "# Keep model inputs safe (avoid 512/1024 token issues)\n",
        "brief_for_dl  = summarize(doc, 3)           # ≤ ~3 sentences\n",
        "snippet_for_zs = doc[:2000]                 # safe chunk for zero-shot\n",
        "\n",
        "# Build pipelines on CPU\n",
        "zs      = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=-1)\n",
        "dl_sent = pipeline(\"sentiment-analysis\",     model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=-1)\n",
        "st      = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1) SENTIMENT, INTENT, EMOTIONS (LLM + DL)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"=\"*100)\n",
        "print(\"1) Sentiment, Intent, Emotions\")\n",
        "print(\"-\"*100)\n",
        "\n",
        "sentiment_labels = [\"positive\", \"neutral\", \"negative\"]\n",
        "intent_labels    = [\"inform\", \"analyze\", \"warn\", \"advocate\", \"criticize\", \"celebrate\"]\n",
        "emotion_labels   = [\"anger\", \"fear\", \"sadness\", \"joy\", \"disgust\", \"surprise\", \"trust\", \"anticipation\"]\n",
        "\n",
        "# LLM (BART MNLI, zero-shot on a safe slice)\n",
        "llm_sent    = zs(snippet_for_zs, sentiment_labels, multi_label=False)\n",
        "llm_intent  = zs(snippet_for_zs, intent_labels,  multi_label=True)\n",
        "llm_emotion = zs(snippet_for_zs, emotion_labels, multi_label=True)\n",
        "\n",
        "# Deep Learning (supervised DistilBERT on short summary)\n",
        "dl_out   = dl_sent(brief_for_dl)[0]\n",
        "dl_label = {\"POSITIVE\": \"positive\", \"NEGATIVE\": \"negative\"}[dl_out[\"label\"]]\n",
        "\n",
        "def top_k(z, k=3): return list(zip(z[\"labels\"][:k], z[\"scores\"][:k]))\n",
        "\n",
        "print(\"LLM — Sentiment (zero-shot):\")\n",
        "print(f\"  {llm_sent['labels'][0]}  (p≈{llm_sent['scores'][0]:.3f})\")\n",
        "print(\"DL  — Sentiment (DistilBERT on 3-sentence summary):\")\n",
        "print(f\"  {dl_label}  (score≈{dl_out['score']:.3f})\\n\")\n",
        "\n",
        "print(\"LLM — Intent (top 3):\")\n",
        "for lab, sc in top_k(llm_intent, 3): print(f\"  {lab:>10s} : {sc:.3f}\")\n",
        "\n",
        "print(\"\\nLLM — Emotions (top 5):\")\n",
        "for lab, sc in top_k(llm_emotion, 5): print(f\"  {lab:>10s} : {sc:.3f}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2) Technology, Aviation, Policies (LLM + DL)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"2) Topic strengths (Technology, Aviation, Policies)\")\n",
        "print(\"-\"*100)\n",
        "\n",
        "topics = {\n",
        "    \"technology\": \"aircraft systems, engineering, manufacturing, software, sensors, design, innovation\",\n",
        "    \"aviation\":   \"airlines, airports, flights, aircraft, pilots, safety, air traffic, crash investigations\",\n",
        "    \"policies\":   \"regulations, government rules, safety policies, compliance, oversight, investigations, penalties\"\n",
        "}\n",
        "\n",
        "# LLM zero-shot topic weights\n",
        "llm_topic = zs(snippet_for_zs, list(topics.keys()), multi_label=True)\n",
        "llm_topic_scores = {k: float(v) for k,v in zip(llm_topic[\"labels\"], llm_topic[\"scores\"])}\n",
        "sum_llm = sum(llm_topic_scores.values()) or 1e-9\n",
        "llm_pct = {k: 100*(llm_topic_scores[k]/sum_llm) for k in topics}\n",
        "\n",
        "print(\"LLM — topic distribution (%):\")\n",
        "for k in topics: print(f\"  {k:>10s}: {llm_pct[k]:6.2f}%\")\n",
        "\n",
        "# DL topic weights via SBERT similarity (on brief summary)\n",
        "doc_emb    = st.encode(brief_for_dl, normalize_embeddings=True)\n",
        "topic_embs = {k: st.encode(v, normalize_embeddings=True) for k,v in topics.items()}\n",
        "sims = {k: float(util.cos_sim(doc_emb, topic_embs[k])) for k in topics}\n",
        "\n",
        "vals = np.array(list(sims.values()), dtype=float)\n",
        "if (vals.max()-vals.min()) < 1e-9:\n",
        "    dl_pct = {k: 100.0/len(sims) for k in sims}\n",
        "else:\n",
        "    mm = (vals - vals.min())/(vals.max()-vals.min()); mm = mm / mm.sum()\n",
        "    dl_pct = {k: float(100*m) for k,m in zip(sims.keys(), mm)}\n",
        "\n",
        "print(\"\\nDL — topic distribution (%):\")\n",
        "for k in topics: print(f\"  {k:>10s}: {dl_pct[k]:6.2f}%\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3) COMPARISON SUMMARY\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"3) LLM vs Deep Learning — Comparison\")\n",
        "print(\"-\"*100)\n",
        "print(\"Sentiment\")\n",
        "print(f\"  LLM: {llm_sent['labels'][0]} (p≈{llm_sent['scores'][0]:.3f})\")\n",
        "print(f\"  DL : {dl_label} (score≈{dl_out['score']:.3f})\")\n",
        "\n",
        "print(\"\\nTopics (% emphasis)\")\n",
        "for k in topics:\n",
        "    print(f\"  {k:>10s} | LLM: {llm_pct[k]:6.2f}%   DL: {dl_pct[k]:6.2f}%\")\n",
        "\n",
        "print(\"\\nIntent (LLM top 3):\", \", \".join([f\"{lab} ({sc:.2f})\" for lab,sc in top_k(llm_intent,3)]))\n",
        "print(\"Emotions (LLM top 5):\", \", \".join([f\"{lab} ({sc:.2f})\" for lab,sc in top_k(llm_emotion,5)]))\n",
        "\n",
        "major_llm = max(llm_pct, key=llm_pct.get)\n",
        "major_dl  = max(dl_pct,  key=dl_pct.get)\n",
        "narr = (\n",
        "    f\"LLM emphasizes **{major_llm}**, while DL similarity emphasizes **{major_dl}**. \"\n",
        "    f\"LLM uses zero-shot reasoning for sentiment/intent/emotions; DL sentiment is supervised \"\n",
        "    f\"and SBERT gives topic similarity. Minor differences come from truncation strategy and model objectives.\"\n",
        ")\n",
        "print(\"\\nSummary:\")\n",
        "print(textwrap.fill(narr, 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u_-ANhHkDah",
        "outputId": "0416e3ac-f6fb-4b21-ed86-c7f99d4898b3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Source\n",
            "----------------------------------------------------------------------------------------------------\n",
            "URL: https://apnews.com/article/boeing-aviation-aircraft-air-india-crashf12b20e65dc57ae655a1e0759b58938f\n",
            "Characters: 43,196\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "1) Sentiment, Intent, Emotions\n",
            "----------------------------------------------------------------------------------------------------\n",
            "LLM — Sentiment (zero-shot):\n",
            "  negative  (p≈0.758)\n",
            "DL  — Sentiment (DistilBERT on 3-sentence summary):\n",
            "  positive  (score≈0.996)\n",
            "\n",
            "LLM — Intent (top 3):\n",
            "        warn : 0.621\n",
            "      inform : 0.547\n",
            "     analyze : 0.467\n",
            "\n",
            "LLM — Emotions (top 5):\n",
            "       anger : 0.530\n",
            "        fear : 0.487\n",
            "     sadness : 0.414\n",
            "       trust : 0.386\n",
            "  anticipation : 0.294\n",
            "\n",
            "====================================================================================================\n",
            "2) Topic strengths (Technology, Aviation, Policies)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "LLM — topic distribution (%):\n",
            "  technology:  21.27%\n",
            "    aviation:  59.52%\n",
            "    policies:  19.21%\n",
            "\n",
            "DL — topic distribution (%):\n",
            "  technology:   1.03%\n",
            "    aviation:  98.97%\n",
            "    policies:   0.00%\n",
            "\n",
            "====================================================================================================\n",
            "3) LLM vs Deep Learning — Comparison\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Sentiment\n",
            "  LLM: negative (p≈0.758)\n",
            "  DL : positive (score≈0.996)\n",
            "\n",
            "Topics (% emphasis)\n",
            "  technology | LLM:  21.27%   DL:   1.03%\n",
            "    aviation | LLM:  59.52%   DL:  98.97%\n",
            "    policies | LLM:  19.21%   DL:   0.00%\n",
            "\n",
            "Intent (LLM top 3): warn (0.62), inform (0.55), analyze (0.47)\n",
            "Emotions (LLM top 5): anger (0.53), fear (0.49), sadness (0.41), trust (0.39), anticipation (0.29)\n",
            "\n",
            "Summary:\n",
            "LLM emphasizes **aviation**, while DL similarity emphasizes **aviation**. LLM uses zero-shot\n",
            "reasoning for sentiment/intent/emotions; DL sentiment is supervised and SBERT gives topic\n",
            "similarity. Minor differences come from truncation strategy and model objectives.\n"
          ]
        }
      ]
    }
  ]
}