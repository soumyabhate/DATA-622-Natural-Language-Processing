{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## DATA 622 Natural Language Processing\n",
        "### Homework 4"
      ],
      "metadata": {
        "id": "8xlnyLXtFdQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions\n",
        "Use the Gettysburg Address by Abraham Lincoln.\n",
        "1. Tokenization\n",
        "Tokenize both sentences into words using spaCy. Print the list of tokens for each sentence.\n",
        "Also use the benepar library.\n",
        "2. Part-of-Speech Tagging\n",
        "Print the part-of-speech (POS) tag for each token in the first sentence.\n",
        "3. Dependency Parsing\n",
        "Print the dependency relation and head word for each token in the second sentence.\n",
        "4. Constituent Parsing\n",
        "Using the NLTK and benepar libraries, print the constituency (phrase structure) parse tree\n",
        "of the first sentence.\n",
        "5. Extract Noun Phrases\n",
        "Using spaCy, extract all noun phrases (noun chunks) from both sentences.\n",
        "5. CRF and HMM\n",
        "Why do you use CRF and HMM? How do they differ? Please summarize in less than 50\n",
        "words."
      ],
      "metadata": {
        "id": "EuF2uFpG_8XB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install benepar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDCI8am9An_N",
        "outputId": "7ee100c6-891d-4046-f5e0-13e7aef2f6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting benepar\n",
            "  Downloading benepar-0.2.0.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.12/dist-packages (from benepar) (3.9.1)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.12/dist-packages (from benepar) (3.8.7)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from benepar) (2.8.0+cu126)\n",
            "Collecting torch-struct>=0.5 (from benepar)\n",
            "  Downloading torch_struct-0.5-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.12/dist-packages (from benepar) (0.22.0)\n",
            "Requirement already satisfied: transformers>=4.2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (4.56.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from benepar) (5.29.5)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from benepar) (0.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2->benepar) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2->benepar) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2->benepar) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2->benepar) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (0.17.4)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=2.0.9->benepar) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.9.4->benepar) (0.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->benepar) (3.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (0.6.2)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (1.10.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[tokenizers,torch]>=4.2.2->benepar) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.9.4->benepar) (1.1.10)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.0.9->benepar) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6.0->benepar) (1.3.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=2.0.9->benepar) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=2.0.9->benepar) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.0.9->benepar) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.0.9->benepar) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy>=2.0.9->benepar) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.0.9->benepar) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.0.9->benepar) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (0.1.2)\n",
            "Downloading torch_struct-0.5-py3-none-any.whl (34 kB)\n",
            "Building wheels for collected packages: benepar\n",
            "  Building wheel for benepar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for benepar: filename=benepar-0.2.0-py3-none-any.whl size=37625 sha256=99c1bd515c01c2d525c4a21ce48a1dbd09c094d118ba5599afd3d6a8d0513fd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/84/c1/f2ac877f519e2864e7dfe52a1c17fe5cdd50819cb8d1f1945f\n",
            "Successfully built benepar\n",
            "Installing collected packages: torch-struct, benepar\n",
            "Successfully installed benepar-0.2.0 torch-struct-0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import benepar, spacy\n",
        "import spacy.cli\n",
        "\n",
        "# Install spacy model if not already\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "\n",
        "# Load spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add benepar\n",
        "benepar.download('benepar_en3')\n",
        "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})"
      ],
      "metadata": {
        "id": "xjoxGVfPAuCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9088c2-61c7-4ea1-c659-399d1b98f49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<benepar.integrations.spacy_plugin.BeneparComponent at 0x7b3f7dd99490>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tokenization (spaCy + benepar)"
      ],
      "metadata": {
        "id": "wz88obvEB6A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import benepar\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add benepar\n",
        "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "# Two sentences\n",
        "sentences = [\n",
        "    \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.\",\n",
        "    \"Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure.\"\n",
        "]\n",
        "\n",
        "# Tokenization\n",
        "for sent in sentences:\n",
        "    doc = nlp(sent)\n",
        "    print([token.text for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9Kb_6oG__BV",
        "outputId": "9b13ad7f-f8bf-472a-ff26-60e4898f5de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.']\n",
            "['Now', 'we', 'are', 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Part-of-Speech Tagging (first sentence)"
      ],
      "metadata": {
        "id": "-tJuItZECANR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(sentences[0])\n",
        "for token in doc1:\n",
        "    print(f\"{token.text} --> {token.pos_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Adv1WcMABp8B",
        "outputId": "280929b3-7bbb-4238-e756-d60ec998989e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Four --> NUM\n",
            "score --> NOUN\n",
            "and --> CCONJ\n",
            "seven --> NUM\n",
            "years --> NOUN\n",
            "ago --> ADV\n",
            "our --> PRON\n",
            "fathers --> NOUN\n",
            "brought --> VERB\n",
            "forth --> ADV\n",
            "on --> ADP\n",
            "this --> DET\n",
            "continent --> NOUN\n",
            ", --> PUNCT\n",
            "a --> DET\n",
            "new --> ADJ\n",
            "nation --> NOUN\n",
            ", --> PUNCT\n",
            "conceived --> VERB\n",
            "in --> ADP\n",
            "Liberty --> PROPN\n",
            ", --> PUNCT\n",
            "and --> CCONJ\n",
            "dedicated --> VERB\n",
            "to --> ADP\n",
            "the --> DET\n",
            "proposition --> NOUN\n",
            "that --> SCONJ\n",
            "all --> DET\n",
            "men --> NOUN\n",
            "are --> AUX\n",
            "created --> VERB\n",
            "equal --> ADJ\n",
            ". --> PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Dependency Parsing (second sentence)"
      ],
      "metadata": {
        "id": "M-Nn6b5aCFI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(sentences[1])\n",
        "for token in doc2:\n",
        "    print(f\"{token.text} --> {token.dep_} --> {token.head.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plSY4RsVCJsT",
        "outputId": "2401c780-24c4-4777-d688-224c6c26acd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now --> advmod --> engaged\n",
            "we --> nsubjpass --> engaged\n",
            "are --> auxpass --> engaged\n",
            "engaged --> ROOT --> engaged\n",
            "in --> prep --> engaged\n",
            "a --> det --> war\n",
            "great --> amod --> war\n",
            "civil --> amod --> war\n",
            "war --> pobj --> in\n",
            ", --> punct --> engaged\n",
            "testing --> advcl --> endure\n",
            "whether --> mark --> conceived\n",
            "that --> det --> nation\n",
            "nation --> nsubj --> conceived\n",
            ", --> punct --> nation\n",
            "or --> cc --> nation\n",
            "any --> det --> nation\n",
            "nation --> conj --> nation\n",
            "so --> advmod --> conceived\n",
            "conceived --> ccomp --> testing\n",
            "and --> cc --> conceived\n",
            "so --> advmod --> dedicated\n",
            "dedicated --> conj --> conceived\n",
            ", --> punct --> endure\n",
            "can --> aux --> endure\n",
            "long --> advmod --> endure\n",
            "endure --> advcl --> engaged\n",
            ". --> punct --> engaged\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Constituent Parsing (first sentence)"
      ],
      "metadata": {
        "id": "4ZEimuPjCOXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(sentences[0])\n",
        "sent = list(doc1.sents)[0]\n",
        "print(sent._.parse_string)   # phrase structure tree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60o45kmRCSLy",
        "outputId": "36a0610a-78d0-4412-b0b0-0921b987001a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP (CD Four) (NN score)) (CC and) (S (ADVP (NP (CD seven) (NNS years)) (RB ago)) (NP (PRP$ our) (NNS fathers)) (VP (VBD brought) (ADVP (RB forth)) (PP (IN on) (NP (DT this) (NN continent))) (, ,) (NP (NP (DT a) (JJ new) (NN nation)) (, ,) (VP (VBN conceived) (PP (IN in) (NP (NNP Liberty))))) (, ,) (CC and) (VP (VBN dedicated) (PP (IN to) (NP (DT the) (NN proposition) (SBAR (IN that) (S (NP (DT all) (NNS men)) (VP (VBP are) (VP (VBN created) (S (ADJP (JJ equal)))))))))))) (. .))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Extract Noun Phrases (noun chunks)"
      ],
      "metadata": {
        "id": "GdFRKboFCV9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sent in enumerate(sentences):\n",
        "    doc = nlp(sent)\n",
        "    print(f\"Sentence {i+1} noun phrases:\")\n",
        "    for chunk in doc.noun_chunks:\n",
        "        print(\"-\", chunk.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZUN_AQuCYfL",
        "outputId": "d0ee61de-704c-477d-850b-e88b224116ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1 noun phrases:\n",
            "- Four score\n",
            "- our fathers\n",
            "- this continent\n",
            "- a new nation\n",
            "- Liberty\n",
            "- the proposition\n",
            "- all men\n",
            "Sentence 2 noun phrases:\n",
            "- we\n",
            "- a great civil war\n",
            "- that nation\n",
            "- any nation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. CRF vs HMM (summary in < 50 words)"
      ],
      "metadata": {
        "id": "C-BOSgjvCgcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HMMs model sequential data using hidden states and transitions, assuming Markov dependence. CRFs directly model conditional probabilities of labels given the sequence, capturing more context. HMMs are generative, CRFs are discriminative — making CRFs more accurate for NLP tasks like POS tagging and NER."
      ],
      "metadata": {
        "id": "U0RwE1YrChXq"
      }
    }
  ]
}